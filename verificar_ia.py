"""
VERIFICACI√ìN Y CONFIGURACI√ìN DE OLLAMA
Script para verificar que la IA est√© funcionando
"""
import os
import sys
import json
import time
import subprocess
import requests
from pathlib import Path

print("="*70)
print(" "*20 + "VERIFICACI√ìN DE IA (OLLAMA)")
print("="*70)
print()

# ========== PASO 1: VERIFICAR SI OLLAMA EST√Å INSTALADO ==========
print("[1] VERIFICANDO INSTALACI√ìN DE OLLAMA")
print("-"*40)

ollama_installed = False
try:
    result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
    if result.returncode == 0:
        print(f"‚úÖ Ollama instalado: {result.stdout.strip()}")
        ollama_installed = True
    else:
        print("‚ùå Ollama no est√° instalado")
except FileNotFoundError:
    print("‚ùå Ollama no est√° instalado")
except Exception as e:
    print(f"‚ùå Error verificando Ollama: {e}")

if not ollama_installed:
    print("\nüì• INSTRUCCIONES PARA INSTALAR OLLAMA:")
    print("   1. Ve a: https://ollama.ai/download")
    print("   2. Descarga Ollama para Windows")
    print("   3. Instala y ejecuta Ollama")
    print("   4. En una terminal, ejecuta: ollama pull deepseek-r1:14b")
    print("\n‚ö†Ô∏è Sin Ollama, el bot NO podr√° tomar decisiones inteligentes")
    
print()

# ========== PASO 2: VERIFICAR SI OLLAMA EST√Å EJECUT√ÅNDOSE ==========
print("[2] VERIFICANDO SERVICIO OLLAMA")
print("-"*40)

ollama_running = False
try:
    response = requests.get("http://localhost:11434/api/tags", timeout=2)
    if response.status_code == 200:
        print("‚úÖ Ollama est√° ejecut√°ndose")
        ollama_running = True
        
        # Listar modelos disponibles
        data = response.json()
        models = data.get("models", [])
        if models:
            print("\nüì¶ MODELOS DISPONIBLES:")
            for model in models:
                name = model.get("name", "")
                size = model.get("size", 0) / (1024**3)  # Convertir a GB
                print(f"   - {name} ({size:.1f} GB)")
        else:
            print("‚ö†Ô∏è No hay modelos descargados")
            
    else:
        print("‚ùå Ollama responde pero con error")
        
except requests.ConnectionError:
    print("‚ùå Ollama NO est√° ejecut√°ndose")
    print("\nüöÄ INTENTANDO INICIAR OLLAMA...")
    
    try:
        # Intentar iniciar Ollama
        subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print("   Esperando 5 segundos...")
        time.sleep(5)
        
        # Verificar de nuevo
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            print("‚úÖ Ollama iniciado correctamente")
            ollama_running = True
        else:
            print("‚ùå No se pudo iniciar Ollama")
            
    except Exception as e:
        print(f"‚ùå Error iniciando Ollama: {e}")
        
except Exception as e:
    print(f"‚ùå Error conectando a Ollama: {e}")

print()

# ========== PASO 3: VERIFICAR MODELO DEEPSEEK ==========
print("[3] VERIFICANDO MODELO DEEPSEEK-R1:14B")
print("-"*40)

model_available = False
if ollama_running:
    try:
        response = requests.get("http://localhost:11434/api/tags")
        data = response.json()
        models = [m.get("name", "") for m in data.get("models", [])]
        
        if "deepseek-r1:14b" in models:
            print("‚úÖ Modelo deepseek-r1:14b disponible")
            model_available = True
        else:
            print("‚ùå Modelo deepseek-r1:14b NO est√° descargado")
            
            # Verificar modelos alternativos
            alt_models = ["llama3.1", "llama3", "mistral", "phi3", "gemma"]
            available_alt = [m for m in alt_models if any(m in model for model in models)]
            
            if available_alt:
                print(f"\nüì¶ MODELOS ALTERNATIVOS DISPONIBLES:")
                for model in available_alt:
                    print(f"   - {model}")
                print("\nüí° Puedes usar uno de estos modelos cambiando OLLAMA_MODEL en configs/.env")
            
            print("\nüì• PARA DESCARGAR DEEPSEEK:")
            print("   Ejecuta en terminal: ollama pull deepseek-r1:14b")
            print("   Nota: El modelo pesa ~8GB")
            
    except Exception as e:
        print(f"‚ùå Error verificando modelos: {e}")
        
print()

# ========== PASO 4: PROBAR LA IA ==========
print("[4] PROBANDO LA IA CON UNA SE√ëAL REAL")
print("-"*40)

if ollama_running:
    from dotenv import load_dotenv
    load_dotenv('configs/.env')
    
    # Configurar cliente
    from openai import OpenAI
    
    api_base = os.getenv("OLLAMA_API_BASE", "http://localhost:11434/v1")
    model = os.getenv("OLLAMA_MODEL", "deepseek-r1:14b")
    
    print(f"Usando modelo: {model}")
    print("Enviando consulta de prueba...")
    
    try:
        client = OpenAI(base_url=api_base, api_key="ollama")
        
        # Prompt de prueba
        test_prompt = {
            "symbol": "BTCUSDm",
            "precio": 118000,
            "tabla": [
                {"tf": "5m", "rsi": 35, "macd_hist": -30, "rvol": 1.5},
                {"tf": "15m", "rsi": 38, "macd_hist": -25, "rvol": 1.3},
                {"tf": "1h", "rsi": 42, "macd_hist": -10, "rvol": 1.1}
            ]
        }
        
        response = client.chat.completions.create(
            model=model,
            temperature=0.2,
            messages=[
                {"role": "system", "content": "Eres un trader. Responde con JSON: {\"senal_final\":\"COMPRA/VENTA/NO OPERAR\", \"confianza\":0.0-1.0, \"razon\":\"texto\"}"},
                {"role": "user", "content": json.dumps(test_prompt)}
            ],
            max_tokens=200
        )
        
        if response and response.choices:
            ai_response = response.choices[0].message.content
            print("\n‚úÖ IA RESPONDI√ì:")
            print("-"*40)
            
            try:
                # Intentar parsear JSON
                json_response = json.loads(ai_response)
                print(f"Se√±al: {json_response.get('senal_final', 'N/A')}")
                print(f"Confianza: {json_response.get('confianza', 0):.1%}")
                print(f"Raz√≥n: {json_response.get('razon', 'N/A')}")
            except:
                # Si no es JSON, mostrar respuesta cruda
                print(ai_response[:200])
                
            print("\nüéâ ¬°LA IA EST√Å FUNCIONANDO CORRECTAMENTE!")
            
        else:
            print("‚ùå La IA no respondi√≥")
            
    except Exception as e:
        print(f"‚ùå Error al probar la IA: {e}")
        
        # Verificar si es problema del modelo
        if "model" in str(e).lower():
            print("\nüí° SOLUCI√ìN:")
            print("   1. Descarga el modelo: ollama pull deepseek-r1:14b")
            print("   2. O usa otro modelo disponible")
            
else:
    print("‚ö†Ô∏è No se puede probar la IA porque Ollama no est√° ejecut√°ndose")

print()

# ========== RESUMEN FINAL ==========
print("="*70)
print(" "*20 + "RESUMEN DE ESTADO")
print("="*70)
print()

status = {
    "Ollama instalado": ollama_installed,
    "Ollama ejecut√°ndose": ollama_running,
    "Modelo disponible": model_available,
    "IA funcionando": ollama_running and model_available
}

all_ok = True
for item, ok in status.items():
    icon = "‚úÖ" if ok else "‚ùå"
    print(f"   {icon} {item}")
    if not ok:
        all_ok = False

print()

if all_ok:
    print("üéâ ¬°TODO LISTO! La IA puede generar se√±ales de trading")
else:
    print("‚ö†Ô∏è Hay problemas con la IA que necesitan resolverse")
    print("\nüìù PASOS PARA SOLUCIONAR:")
    
    if not ollama_installed:
        print("   1. Instalar Ollama desde https://ollama.ai/download")
        
    if not ollama_running:
        print("   2. Ejecutar Ollama: ollama serve")
        
    if not model_available:
        print("   3. Descargar modelo: ollama pull deepseek-r1:14b")

print()
print("üí° NOTA: El bot puede funcionar sin IA, pero NO tomar√° decisiones inteligentes")
print()

input("Presiona Enter para salir...")
