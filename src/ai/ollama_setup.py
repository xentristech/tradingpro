"""
SCRIPT COMPLETO DE VERIFICACI√ìN Y CONFIGURACI√ìN DE OLLAMA
"""
import os
import sys
import subprocess
import time
import json
import platform
from pathlib import Path

def print_header(text):
    print("="*70)
    print(f"   {text}")
    print("="*70)
    print()

def check_command_exists(command):
    """Verifica si un comando existe en el sistema"""
    try:
        subprocess.run([command, "--version"], capture_output=True, timeout=3)
        return True
    except:
        return False

def test_http_connection():
    """Prueba la conexi√≥n HTTP a Ollama"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        return response.status_code == 200, response.json() if response.status_code == 200 else None
    except:
        return False, None

def find_ollama_installation():
    """Busca d√≥nde est√° instalado Ollama"""
    possible_paths = [
        Path.home() / "AppData/Local/Programs/Ollama/ollama.exe",
        Path.home() / "AppData/Local/Ollama/ollama.exe",
        Path("C:/Program Files/Ollama/ollama.exe"),
        Path("C:/ollama/ollama.exe"),
    ]
    
    for path in possible_paths:
        if path.exists():
            return str(path)
    return None

def main():
    print_header("VERIFICACI√ìN COMPLETA DE OLLAMA PARA TRADING BOT")
    
    # 1. Sistema Operativo
    print("üìã INFORMACI√ìN DEL SISTEMA")
    print(f"   OS: {platform.system()} {platform.release()}")
    print(f"   Python: {sys.version.split()[0]}")
    print()
    
    # 2. Verificar instalaci√≥n
    print("üîç VERIFICANDO INSTALACI√ìN DE OLLAMA")
    print("-"*40)
    
    ollama_in_path = check_command_exists("ollama")
    ollama_path = find_ollama_installation()
    
    if ollama_in_path:
        print("‚úÖ Ollama est√° instalado y en el PATH")
        
        # Obtener versi√≥n
        try:
            result = subprocess.run(["ollama", "--version"], capture_output=True, text=True)
            print(f"   Versi√≥n: {result.stdout.strip()}")
        except:
            pass
            
    elif ollama_path:
        print(f"‚ö†Ô∏è Ollama encontrado pero no en PATH: {ollama_path}")
        print("   A√±ade Ollama al PATH del sistema para usar desde cualquier lugar")
    else:
        print("‚ùå Ollama NO est√° instalado")
        print("\n" + "="*70)
        print("üì• INSTRUCCIONES PARA INSTALAR OLLAMA:")
        print("="*70)
        print("""
1. Abre tu navegador y ve a:
   üåê https://ollama.ai/download

2. Haz clic en "Download for Windows"

3. Ejecuta el instalador descargado (OllamaSetup.exe)

4. Sigue las instrucciones del instalador

5. Despu√©s de instalar:
   - Abre una terminal nueva (CMD o PowerShell)
   - Ejecuta: ollama serve
   
6. Descarga un modelo (en otra terminal):
   - Para trading (8GB): ollama pull deepseek-r1:14b
   - Alternativa ligera (4.7GB): ollama pull llama3
   - M√°s ligero (2.3GB): ollama pull phi3

7. Vuelve a ejecutar este script
        """)
        return
    
    print()
    
    # 3. Verificar si est√° ejecut√°ndose
    print("üîå VERIFICANDO SERVICIO OLLAMA")
    print("-"*40)
    
    is_running, api_data = test_http_connection()
    
    if is_running:
        print("‚úÖ Ollama est√° EJECUT√ÅNDOSE en http://localhost:11434")
        
        if api_data and "models" in api_data:
            models = api_data["models"]
            print(f"\nüì¶ MODELOS INSTALADOS: {len(models)}")
            
            if models:
                print("-"*40)
                for model in models:
                    name = model.get("name", "desconocido")
                    size_gb = model.get("size", 0) / (1024**3)
                    modified = model.get("modified_at", "")[:10]
                    print(f"   ‚Ä¢ {name}")
                    print(f"     Tama√±o: {size_gb:.1f} GB | Modificado: {modified}")
                    
                # Verificar modelos recomendados
                model_names = [m.get("name", "") for m in models]
                
                recommended = {
                    "deepseek-r1:14b": "Mejor para trading (8GB)",
                    "deepseek-r1": "Versi√≥n general DeepSeek",
                    "llama3.1": "Buena alternativa (4.7GB)",
                    "llama3": "Alternativa estable",
                    "mistral": "R√°pido y eficiente (4.1GB)",
                    "phi3": "M√°s ligero (2.3GB)"
                }
                
                print("\nüéØ MODELOS RECOMENDADOS PARA TRADING:")
                print("-"*40)
                
                found_recommended = False
                for model_key, description in recommended.items():
                    if any(model_key in name for name in model_names):
                        print(f"   ‚úÖ {model_key}: {description}")
                        found_recommended = True
                        
                if not found_recommended:
                    print("   ‚ö†Ô∏è No tienes ning√∫n modelo recomendado")
                    print("\n   üì• Instala uno con:")
                    print("      ollama pull deepseek-r1:14b  (mejor)")
                    print("      ollama pull llama3  (alternativa)")
                    
            else:
                print("   ‚ö†Ô∏è No hay modelos instalados")
                print("\nüì• INSTALA UN MODELO:")
                print("   ollama pull deepseek-r1:14b  (8GB, mejor para trading)")
                print("   ollama pull llama3  (4.7GB, buena alternativa)")
                print("   ollama pull phi3  (2.3GB, m√°s ligero)")
                
    else:
        print("‚ùå Ollama NO est√° ejecut√°ndose")
        print("\nüöÄ PARA INICIAR OLLAMA:")
        print("-"*40)
        print("1. Abre una terminal nueva (CMD o PowerShell)")
        print("2. Ejecuta: ollama serve")
        print("3. Deja esa terminal abierta")
        print("4. Vuelve a ejecutar este script")
        
        if ollama_in_path or ollama_path:
            print("\nüí° Intentando iniciar Ollama autom√°ticamente...")
            
            try:
                # Intentar iniciar Ollama
                cmd = "ollama" if ollama_in_path else ollama_path
                subprocess.Popen([cmd, "serve"], 
                               stdout=subprocess.DEVNULL,
                               stderr=subprocess.DEVNULL)
                
                print("   Esperando 5 segundos...")
                time.sleep(5)
                
                # Verificar de nuevo
                is_running, _ = test_http_connection()
                if is_running:
                    print("   ‚úÖ ¬°Ollama iniciado correctamente!")
                else:
                    print("   ‚ùå Ollama no respondi√≥ despu√©s de iniciar")
                    print("   Intenta iniciarlo manualmente")
            except Exception as e:
                print(f"   ‚ùå Error al iniciar: {e}")
    
    print()
    
    # 4. Verificar archivo .env
    print("‚öôÔ∏è VERIFICANDO CONFIGURACI√ìN DEL BOT")
    print("-"*40)
    
    env_file = Path("configs/.env")
    if env_file.exists():
        with open(env_file, 'r') as f:
            env_content = f.read()
            
        if "OLLAMA_API_BASE" in env_content:
            print("‚úÖ Configuraci√≥n de Ollama encontrada en .env")
            
            # Extraer valores
            for line in env_content.split('\n'):
                if line.startswith("OLLAMA_MODEL"):
                    model = line.split('=')[1].strip()
                    print(f"   Modelo configurado: {model}")
                    
                    # Verificar si el modelo est√° instalado
                    if is_running and api_data:
                        model_names = [m.get("name", "") for m in api_data.get("models", [])]
                        if any(model in name for name in model_names):
                            print(f"   ‚úÖ Modelo {model} est√° instalado")
                        else:
                            print(f"   ‚ö†Ô∏è Modelo {model} NO est√° instalado")
                            print(f"      Inst√°lalo con: ollama pull {model}")
        else:
            print("‚ö†Ô∏è No hay configuraci√≥n de Ollama en .env")
    else:
        print("‚ùå No existe el archivo configs/.env")
    
    print()
    
    # 5. Resumen final
    print_header("RESUMEN Y RECOMENDACIONES")
    
    if ollama_in_path and is_running:
        print("üéâ ¬°OLLAMA EST√Å LISTO PARA USAR!")
        print("\n‚úÖ Tu bot puede usar IA para tomar decisiones inteligentes")
        print("\nüí° SIGUIENTE PASO:")
        print("   Ejecuta el bot con: .\\EJECUTAR_SISTEMA.bat")
        
    elif ollama_in_path and not is_running:
        print("‚ö†Ô∏è OLLAMA INSTALADO PERO NO EJECUT√ÅNDOSE")
        print("\nüí° ACCI√ìN REQUERIDA:")
        print("   1. Abre una terminal")
        print("   2. Ejecuta: ollama serve")
        print("   3. Deja la terminal abierta")
        print("   4. Ejecuta el bot")
        
    else:
        print("‚ùå OLLAMA NO EST√Å INSTALADO")
        print("\nüí° ACCI√ìN REQUERIDA:")
        print("   1. Ve a https://ollama.ai/download")
        print("   2. Descarga e instala Ollama")
        print("   3. Ejecuta: ollama serve")
        print("   4. Descarga un modelo: ollama pull llama3")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå Error inesperado: {e}")
    
    print()
    input("Presiona Enter para salir...")
